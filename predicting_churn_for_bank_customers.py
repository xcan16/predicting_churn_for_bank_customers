# -*- coding: utf-8 -*-
"""Predicting_Churn_for_Bank_Customers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xhCWzxOt1r-A_-ZYDCtEwGw5wfD-EAH1

# Customer Churn Prediction for a Bank

## Introduction
This project focuses on predicting **customer churn** in a retail bank dataset.  
The goal is to identify customers who are likely to leave the bank so that proactive retention strategies can be designed.

- **Dataset**: `Churn_Modelling.csv`  
- **Target variable**: `Exited` (1 = churned, 0 = stayed)  
- **Techniques used**: Exploratory Data Analysis (EDA), Machine Learning (XGBoost), Explainability (SHAP), and Model Deployment (joblib).  
- **Tools**: Python, Pandas, Scikit-learn, XGBoost, SHAP, Matplotlib/Seaborn.  

This notebook is structured to demonstrate:
1. Data exploration and preprocessing
2. Model building with a reproducible pipeline
3. Model evaluation and threshold optimization
4. Interpretability with SHAP
5. Saving and reusing the trained model

---
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, precision_recall_curve, 
                           f1_score, roc_curve, auc)
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from scipy.stats import randint, uniform
import shap
import joblib

# Read the data
df = pd.read_csv("Churn_Modelling.csv")

df.info()

df.describe().T

df.isna().sum().sort_values(ascending=False)

cat_cols=['Geography','Gender','HasCrCard','IsActiveMember','NumOfProducts']
num_cols=['CreditScore','Age','Tenure','Balance','EstimatedSalary']

"""## Exploratory Data Analysis (EDA)

Before building the predictive model, it is important to understand the dataset.  
In this section, we:

- Split the features into **categorical** (`Geography`, `Gender`, `HasCrCard`, `IsActiveMember`, `NumOfProducts`) and **numerical** (`CreditScore`, `Age`, `Tenure`, `Balance`, `EstimatedSalary`) variables.
- Checked the unique values and distributions of categorical features.
- Examined statistical summaries (min, median, mean, max) for numerical features.

This step helps to identify potential patterns, class imbalances, or unusual distributions that may affect the model.

"""

df[cat_cols].nunique(), df[num_cols].agg(["min","median","mean","max"]).T

for col in ["Geography","Gender","HasCrCard","IsActiveMember","NumOfProducts"]:
  print(f"\n==={col}===")
  print(pd.crosstab(df[col], df["Exited"], normalize="index")*100)

df.groupby("Exited")["Age"].describe().T

df.groupby("Exited")["CreditScore"].describe().T

df.groupby("Exited")["Balance"].describe().T

"""## Visual Explorations

To better understand churn behavior, we visualized several feature distributions:

- **Age (Histogram)**: Comparison between churned and non-churned customers. Older customers show a higher churn tendency.  
- **Balance (Boxplot)**: Distribution of account balances split by churn status. Churned customers tend to cluster at specific balance levels.  
- **NumOfProducts (Stacked Bar)**: Product usage strongly correlates with churn; customers with fewer products are more likely to leave.  
- **Geography (Stacked Bar)**: Country differences highlight Germany as having the highest churn rate.  
- **Correlation Heatmap**: Shows linear relationships among numerical variables (e.g., Age, CreditScore, Balance).  

These visuals provide early insights into customer segments that are most at risk of leaving the bank, guiding feature selection and model design.

"""

import matplotlib.pyplot as plt
df.boxplot(column="CreditScore", by="Exited", grid=False )
plt.title("CreditScore vs Exited")
plt.suptitle("")
plt.savefig('figures/creditscore_vs_exited.png', bbox_inches='tight')
plt.show()

df.boxplot(column="Balance", by="Exited", grid=False)
plt.title("Balance vs Exited")
plt.suptitle("")
plt.savefig('figures/balance_vs_exited.png', bbox_inches='tight')
plt.show()

import seaborn as sns

num_cols = ["CreditScore","Age","Tenure","Balance","EstimatedSalary"]
corr = df[num_cols].corr()

plt.figure(figsize=(6,5))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="viridis")
plt.title("Correlation Heatmap")
plt.savefig('figures/correlation_heatmap.png', bbox_inches='tight')
plt.show()

plt.figure(figsize=(7,4))
plt.hist(df.loc[df["Exited"]==0,"Age"], bins=25, density=True, alpha=0.6, label="Stayed")
plt.hist(df.loc[df["Exited"]==1,"Age"], bins=25, density=True, alpha=0.6, label="Churned")
plt.xlabel("Age")
plt.ylabel("Density")
plt.title("Age Distribution by Churn")
plt.legend()
plt.grid(axis="y")
plt.savefig('figures/age_distribution.png', bbox_inches='tight')
plt.show()

data0 = df.loc[df["Exited"]==0,"Balance"].values
data1 = df.loc[df["Exited"]==1,"Balance"].values
plt.figure(figsize=(7,4))
plt.boxplot([data0, data1], labels=["Stayed","Churned"], showfliers=False)
plt.ylabel("Balance")
plt.title("Balance by Churn (Boxplot)")
plt.grid(axis="y")
plt.savefig('figures/balance_boxplot.png', bbox_inches='tight')
plt.show()

counts = df.groupby(["NumOfProducts","Exited"]).size().unstack(fill_value=0).reindex(sorted(df["NumOfProducts"].unique()))
stayed = counts.get(0, 0).values
churned = counts.get(1, 0).values
x = range(len(counts.index))
plt.figure(figsize=(7,4))
plt.bar(x, stayed, label="Stayed")
plt.bar(x, churned, bottom=stayed, label="Churned")
plt.xticks(x, counts.index)
plt.xlabel("NumOfProducts")
plt.ylabel("Count")
plt.title("NumOfProducts vs Churn (Stacked)")
plt.legend()
plt.grid(axis="y")
plt.savefig('figures/num_products_stacked.png', bbox_inches='tight')
plt.show()

geo = ["France","Germany","Spain"]
tab = df.groupby("Geography")["Exited"].agg(["mean","count"]).reindex(geo)
rate = tab["mean"].values
stay_rate = 1 - rate
x = range(len(geo))
plt.figure(figsize=(7,4))
plt.bar(x, stay_rate, label="Stayed rate")
plt.bar(x, rate, bottom=stay_rate, label="Churn rate")
plt.xticks(x, geo)
plt.ylabel("Rate")
plt.title("Churn Rate by Geography (Stacked)")
plt.ylim(0,1)
plt.legend()
plt.grid(axis="y")
plt.savefig('figures/geography_churn_rate.png', bbox_inches='tight')
plt.show()

"""## Train–Test Split and Class Balance

To evaluate model performance reliably, the dataset is split into **training (80%)** and **testing (20%)** sets, using stratified sampling to preserve the churn distribution.

Key points:
- **Target variable**: `Exited` (0 = stayed, 1 = churned).  
- **Stratification** ensures the same churn ratio is maintained in both train and test sets.  
- Since churn cases (positive class) are fewer than non-churned, the dataset is **imbalanced**, which requires special handling in modeling (e.g., `scale_pos_weight` in XGBoost).  

"""

# Prepare the data
X = df.drop(columns=["RowNumber", "CustomerId", "Surname", "Exited"])
y = df["Exited"]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

print("Train class ratio:")
print((y_train.value_counts(normalize=True)*100).round(2))
print("\nTest class ratio:")
print((y_test.value_counts(normalize=True)*100).round(2))

"""## Modeling with XGBoost Pipeline

For the predictive model, we use **XGBoost (Extreme Gradient Boosting)** — a powerful algorithm well-suited for imbalanced classification tasks like churn prediction.  

Key design choices:
- **ColumnTransformer**:  
  - OneHotEncoder for categorical variables (`Geography`, `Gender`)  
  - StandardScaler for numerical variables  
- **Pipeline**:  
  - Ensures preprocessing and modeling are combined into one reproducible workflow  
  - Prevents data leakage between training and testing  
- **Class imbalance handling**:  
  - The `scale_pos_weight` parameter is set based on the ratio of non-churned to churned customers  

This integrated pipeline makes the model cleaner, repeatable, and production-ready.
"""

# Model Pipeline and Hyperparameter Optimization
print("\nPreparing the model pipeline...")
categorical_cols = ["Geography", "Gender"]
numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("num", StandardScaler(), numeric_cols),
    ]
)

# XGBoost base model pipeline
base_model = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", XGBClassifier(
        random_state=42,
        use_label_encoder=False,
        eval_metric="logloss",
        scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1]
    ))
])

# Hyperparameter optimization
print("\nStarting hyperparameter optimization...")
param_dist = {
    "classifier__n_estimators": randint(300, 900),
    "classifier__max_depth": randint(3, 9),
    "classifier__learning_rate": uniform(0.01, 0.15),
    "classifier__subsample": uniform(0.6, 0.4),
    "classifier__colsample_bytree": uniform(0.6, 0.4),
    "classifier__min_child_weight": randint(1, 8),
    "classifier__gamma": uniform(0.0, 0.4),
    "classifier__reg_lambda": uniform(0.5, 1.5),
    "classifier__reg_alpha": uniform(0.0, 0.5)
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
search = RandomizedSearchCV(
    estimator=base_model,
    param_distributions=param_dist,
    n_iter=30,
    scoring="roc_auc",
    n_jobs=-1,
    cv=cv,
    verbose=1,
    random_state=42
)

# Find the best model
search.fit(X_train, y_train)
model = search.best_estimator_

print("\nBest parameters:")
for param, value in search.best_params_.items():
    print(f"{param}: {value}")

# Data transformations
X_train_transformed = model.named_steps["preprocessor"].transform(X_train)
X_test_transformed = model.named_steps["preprocessor"].transform(X_test)

"""## Model Evaluation

After training the XGBoost pipeline, we evaluate the model on the test set using multiple metrics:

- **Confusion Matrix**: Shows the distribution of true positives, false positives, true negatives, and false negatives.  
- **Classification Report**: Provides precision, recall, and F1-score for each class (churned vs. stayed).  
- **ROC-AUC**: Measures the ability of the model to distinguish between churned and non-churned customers.  

These metrics provide a holistic view of how well the model balances accuracy and the detection of churn cases.

"""

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Tahminler yapılıyor
print("\nMaking predictions...")
y_pred = model.predict(X_test)
y_probs = model.predict_proba(X_test)[:,1]

print("\nModel Evaluation Results:")
print("-----------------------------")
print("ROC-AUC:", roc_auc_score(y_test, y_probs))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Farklı eşik değerleriyle sonuçları değerlendir
# Eşik = 0.5
y_pred_05 = (y_probs >= 0.5).astype(int)
print("\nClassification Report (threshold=0.5):")
print(classification_report(y_test, y_pred_05))
print("\nConfusion Matrix (threshold=0.5):")
print(confusion_matrix(y_test, y_pred_05))

# Threshold = 0.4
y_pred_04 = (y_probs >= 0.4).astype(int)

print("\nClassification Report (threshold=0.4):")
print(classification_report(y_test, y_pred_04))
print("\nConfusion Matrix (threshold=0.4):")
print(confusion_matrix(y_test, y_pred_04))

# Threshold = 0.6
y_pred_06 = (y_probs >= 0.6).astype(int)

print("\nClassification Report (threshold=0.6):")
print(classification_report(y_test, y_pred_06))
print("\nConfusion Matrix (threshold=0.6):")
print(confusion_matrix(y_test, y_pred_06))

from sklearn.ensemble import RandomForestClassifier

# Model Karşılaştırması
print("\nComparing models...")

# Random Forest model
rf_model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        random_state=42,
        class_weight="balanced"
    ))
])

# Random Forest eğitimi ve tahminler
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_probs = rf_model.predict_proba(X_test)[:,1]

# XGBoost tahminleri
xgb_pred = model.predict(X_test)
xgb_probs = model.predict_proba(X_test)[:,1]

# Model karşılaştırma sonuçları
results = pd.DataFrame({
    'Model': ['XGBoost', 'Random Forest'],
    'ROC-AUC': [roc_auc_score(y_test, xgb_probs), 
                roc_auc_score(y_test, rf_probs)],
    'Accuracy': [(xgb_pred == y_test).mean(), 
                (rf_pred == y_test).mean()],
    'F1-Score': [f1_score(y_test, xgb_pred), 
                 f1_score(y_test, rf_pred)]
})

print("\nModel Comparison Results:")
print(results.round(4))

# En iyi modelin detaylı sonuçları
print("\nBest Model (XGBoost) Detailed Results:")
print("\nClassification Report:")
print(classification_report(y_test, xgb_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, xgb_pred))

import numpy as np
pos=(y_train==1).sum()
neg=(y_train==0).sum()
scale_pos_weight=neg/pos
scale_pos_weight

# Preprocessing definition
categorical_cols = ["Geography", "Gender"]
numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("num", StandardScaler(), numeric_cols),
    ]
)

# Model pipeline definition
pipe = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", XGBClassifier(
        random_state=42,
        use_label_encoder=False,
        eval_metric="logloss",
        scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1]
    ))
])

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from scipy.stats import randint, uniform

param_dist = {
    "classifier__n_estimators": randint(300, 900),
    "classifier__max_depth": randint(3, 9),
    "classifier__learning_rate": uniform(0.01, 0.15),
    "classifier__subsample": uniform(0.6, 0.4),
    "classifier__colsample_bytree": uniform(0.6, 0.4),
    "classifier__min_child_weight": randint(1, 8),
    "classifier__gamma": uniform(0.0, 0.4),
    "classifier__reg_lambda": uniform(0.5, 1.5),
    "classifier__reg_alpha": uniform(0.0, 0.5)
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rs = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    n_iter=30,
    scoring="roc_auc",
    n_jobs=-1,
    cv=cv,
    verbose=1,
    random_state=42
)

rs.fit(X_train, y_train)

print("Best CV ROC-AUC:", rs.best_score_)
print("Best Params:")
for k, v in rs.best_params_.items():
    print(f"  {k}: {v}")

best_model = rs.best_estimator_
y_pred  = best_model.predict(X_test)
y_probs = best_model.predict_proba(X_test)[:, 1]

print("Test ROC-AUC:", roc_auc_score(y_test, y_probs))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# This part was already done above


categorical_cols = ["Geography", "Gender"]
numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("num", StandardScaler(), numeric_cols),
    ]
)

xgb_model = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("classifier", XGBClassifier(
            random_state=42,
            use_label_encoder=False,
            eval_metric="logloss",
            scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1]
        ))
    ]
)

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score


# XGBoost modelini eğit ve tahminleri al
print("Training XGBoost model...")
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
y_probs = xgb_model.predict_proba(X_test)[:, 1]
print("XGBoost model training and predictions completed.\n")

print("Model Evaluation Results:")
print("-----------------------------")
print("ROC-AUC:", roc_auc_score(y_test, y_probs))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""## Threshold Optimization

While models output probabilities, business decisions usually need a binary label.  
Using the default threshold of **0.50** can be sub-optimal for **imbalanced** problems like churn.

In this step we:
- Plot **Precision** and **Recall** against the decision threshold to visualize the trade-off.
- Select a threshold that aligns with the business objective (e.g., prioritize recall to catch more churners, or balance with F1).
- Report performance again **at the chosen threshold** for transparency and reproducibility.

This makes the evaluation actionable: stakeholders can tune sensitivity (recall) vs. cost (precision) based on operational constraints.

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve


precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)

plt.figure(figsize=(7,5))
plt.plot(thresholds, precisions[:-1], label="Precision")
plt.plot(thresholds, recalls[:-1], label="Recall")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision-Recall vs Threshold")
plt.legend()
plt.grid()
plt.savefig('figures/precision_recall_threshold.png', bbox_inches='tight')
plt.show()


target_recall = 0.75
idx = np.argmax(recalls >= target_recall)
best_threshold = thresholds[idx]

print("Selected  threshold:", best_threshold)

y_pred_thresh = (y_probs >= best_threshold).astype(int)
print("\nClassification Report (New Threshold):\n", classification_report(y_test, y_pred_thresh))

from sklearn.metrics import f1_score

f1_scores = []
for t in thresholds:
    y_pred_t = (y_probs >= t).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_t))

best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]

print("Best threshold (based on F1):", best_threshold)


y_pred_best = (y_probs >= best_threshold).astype(int)
print("\nClassification Report (Best threshold):\n", classification_report(y_test, y_pred_best))
print("ROC-AUC:", roc_auc_score(y_test, y_probs))

"""## Model Interpretability with SHAP

We use **SHAP (SHapley Additive exPlanations)** to explain individual predictions and the model globally:
- **What it shows**: the contribution of each feature to the churn probability for each sample.
- **Sign**: positive SHAP values push predictions toward **churn (1)**; negative values push toward **stay (0)**.
- **Global view**:
  - **Bar plot** ranks features by average absolute impact.
  - **Dot plot** shows the distribution and direction of impacts across samples.

Notes and caveats:
- SHAP attributions are **model- and data-dependent**; correlated features may split/shared importance.
- Interpret patterns, not single points: combine SHAP with domain knowledge and business context.

These insights guide actionable policies (e.g., product bundling for low-product customers, tailored offers by geography/age).

"""

import shap


# SHAP Değerleri ve Özellik Önem Analizi
print("\nCalculating SHAP values...")
feature_names = model.named_steps["preprocessor"].get_feature_names_out()
explainer = shap.Explainer(model.named_steps["classifier"])
shap_values = explainer(X_test_transformed)

# SHAP visualizations
print("Creating SHAP visualizations...")
plt.figure(figsize=(10,6))
shap.summary_plot(shap_values, X_test_transformed, plot_type="bar", feature_names=feature_names)
plt.savefig('figures/shap_importance_bar.png', bbox_inches='tight')
plt.close()

plt.figure(figsize=(10,8))
shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names)
plt.savefig('figures/shap_importance_dot.png', bbox_inches='tight')
plt.close()

# Get feature names after preprocessor is fitted
feature_names = preprocessor.get_feature_names_out()
shap_importance = np.abs(shap_values.values).mean(axis=0)

importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Mean(|SHAP|)": shap_importance
}).sort_values(by="Mean(|SHAP|)", ascending=False)

importance_df.head(15)

plt.figure(figsize=(8,6))
plt.barh(importance_df["Feature"][:15][::-1], importance_df["Mean(|SHAP|)"][:15][::-1])
plt.xlabel("Mean(|SHAP value|)")
plt.title("Feature Importance (based on SHAP values)")
plt.grid(axis="x")
plt.savefig('figures/feature_importance.png', bbox_inches='tight')
plt.show()

"""## ROC Curve

The ROC curve shows the trade-off between **True Positive Rate (TPR/Recall)** and **False Positive Rate (FPR)** across all thresholds.
- **AUC (Area Under the Curve)** summarizes overall separability: closer to **1.0** is better.
- A curve well above the diagonal baseline indicates the model is effectively distinguishing churners from non-churners.
- This plot complements Precision–Recall analysis by focusing on ranking quality rather than a single operating point.

We report the **AUC** on the test set and visualize the ROC curve to provide a standard, model-agnostic comparison metric.

"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC curve (AUC = {roc_auc:.3f})")
plt.plot([0,1], [0,1], color="navy", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC Curve)")
plt.legend(loc="lower right")
plt.grid()
plt.savefig('figures/roc_curve.png', bbox_inches='tight')
plt.show()

"""## Feature Importance (Table)

We report global feature importance using the **mean absolute SHAP values**.  
This table complements the SHAP plots by providing a numeric ranking that can be easily cited in reports.

"""

import pandas as pd
import numpy as np


shap_importance = np.abs(shap_values.values).mean(axis=0)


feature_names = xgb_model.named_steps["preprocessor"].get_feature_names_out()


importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Mean(|SHAP|)": shap_importance
}).sort_values(by="Mean(|SHAP|)", ascending=False)

importance_df.head(15)

plt.figure(figsize=(8,6))
plt.barh(importance_df["Feature"][:15][::-1], importance_df["Mean(|SHAP|)"][:15][::-1])
plt.xlabel("Mean(|SHAP value|)")
plt.title("Feature Importance (based on SHAP values)")
plt.grid(axis="x")
plt.savefig('figures/feature_importance.png', bbox_inches='tight')
plt.show()

"""## Model Saving & Reusability

The entire preprocessing + XGBoost pipeline is saved with `joblib` for reproducible inference.  
This enables direct `predict` on new data without retraining, ensuring consistent preprocessing and model behavior.

"""

import joblib

# En iyi modeli kaydet (Hyperparameter optimization sonrası)
print("\nSaving the best model...")
joblib.dump(model, "churn_model.pkl")
print("Model successfully saved: churn_model.pkl")

# Test için modeli yükle ve tahmin yap
print("\nTesting the model...")
loaded_model = joblib.load("churn_model.pkl")
sample_pred = loaded_model.predict(X_test[:5])
print("Sample predictions:", sample_pred)

def plot_model_performance(y_true, y_prob, thresholds, save_path=None):
    """Visualizes model performance metrics."""
    # Precision-Recall vs Threshold
    precisions, recalls, _ = precision_recall_curve(y_true, y_prob)
    
    plt.figure(figsize=(10, 6))
    plt.plot(thresholds, precisions[:-1], 'b-', label='Precision')
    plt.plot(thresholds, recalls[:-1], 'r-', label='Recall')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title('Precision-Recall vs Threshold')
    plt.legend()
    plt.grid(True)
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.close()

def plot_roc_curve(y_true, y_prob, save_path=None):
    """Plots the ROC curve."""
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], 'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.grid(True)
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.close()

def plot_feature_importance(importance_df, n_features=15, save_path=None):
    """Plots the feature importance graph."""
    plt.figure(figsize=(10, 8))
    plt.barh(importance_df['Feature'][:n_features][::-1], 
            importance_df['Mean(|SHAP|)'][:n_features][::-1])
    plt.xlabel('Mean(|SHAP value|)')
    plt.title('Feature Importance')
    plt.grid(axis='x')
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.close()