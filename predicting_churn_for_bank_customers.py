# -*- coding: utf-8 -*-
"""Predicting_Churn_for_Bank_Customers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xhCWzxOt1r-A_-ZYDCtEwGw5wfD-EAH1

# Customer Churn Prediction for a Bank

## Introduction
This project focuses on predicting **customer churn** in a retail bank dataset.  
The goal is to identify customers who are likely to leave the bank so that proactive retention strategies can be designed.

- **Dataset**: `Churn_Modelling.csv`  
- **Target variable**: `Exited` (1 = churned, 0 = stayed)  
- **Techniques used**: Exploratory Data Analysis (EDA), Machine Learning (XGBoost), Explainability (SHAP), and Model Deployment (joblib).  
- **Tools**: Python, Pandas, Scikit-learn, XGBoost, SHAP, Matplotlib/Seaborn.  

This notebook is structured to demonstrate:
1. Data exploration and preprocessing
2. Model building with a reproducible pipeline
3. Model evaluation and threshold optimization
4. Interpretability with SHAP
5. Saving and reusing the trained model

---
"""

from google.colab import files
uploded_files=files.upload()

import numpy as np
import pandas as pd

df=pd.read_csv("Churn_Modelling.csv")

df.info()

df.describe().T

df.isna().sum().sort_values(ascending=False)

cat_cols=['Geography','Gender','HasCrCard','IsActiveMember','NumOfProducts']
num_cols=['CreditScore','Age','Tenure','Balance','EstimatedSalary']

"""## Exploratory Data Analysis (EDA)

Before building the predictive model, it is important to understand the dataset.  
In this section, we:

- Split the features into **categorical** (`Geography`, `Gender`, `HasCrCard`, `IsActiveMember`, `NumOfProducts`) and **numerical** (`CreditScore`, `Age`, `Tenure`, `Balance`, `EstimatedSalary`) variables.
- Checked the unique values and distributions of categorical features.
- Examined statistical summaries (min, median, mean, max) for numerical features.

This step helps to identify potential patterns, class imbalances, or unusual distributions that may affect the model.

"""

df[cat_cols].nunique(), df[num_cols].agg(["min","median","mean","max"]).T

for col in ["Geography","Gender","HasCrCard","IsActiveMember","NumOfProducts"]:
  print(f"\n==={col}===")
  print(pd.crosstab(df[col], df["Exited"], normalize="index")*100)

df.groupby("Exited")["Age"].describe().T

df.groupby("Exited")["CreditScore"].describe().T

df.groupby("Exited")["Balance"].describe().T

"""## Visual Explorations

To better understand churn behavior, we visualized several feature distributions:

- **Age (Histogram)**: Comparison between churned and non-churned customers. Older customers show a higher churn tendency.  
- **Balance (Boxplot)**: Distribution of account balances split by churn status. Churned customers tend to cluster at specific balance levels.  
- **NumOfProducts (Stacked Bar)**: Product usage strongly correlates with churn; customers with fewer products are more likely to leave.  
- **Geography (Stacked Bar)**: Country differences highlight Germany as having the highest churn rate.  
- **Correlation Heatmap**: Shows linear relationships among numerical variables (e.g., Age, CreditScore, Balance).  

These visuals provide early insights into customer segments that are most at risk of leaving the bank, guiding feature selection and model design.

"""

import matplotlib.pyplot as plt
df.boxplot(column="CreditScore", by="Exited", grid=False )
plt.title("CreditScore vs Exited")
plt.suptitle("")
plt.show()

df.boxplot(column="Balance", by="Exited", grid=False)
plt.title("Balance vs Exited")
plt.suptitle("")
plt.show()

import seaborn as sns

num_cols = ["CreditScore","Age","Tenure","Balance","EstimatedSalary"]
corr = df[num_cols].corr()

plt.figure(figsize=(6,5))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="viridis")
plt.title("Correlation Heatmap")
plt.show()

plt.figure(figsize=(7,4))
plt.hist(df.loc[df["Exited"]==0,"Age"], bins=25, density=True, alpha=0.6, label="Stayed")
plt.hist(df.loc[df["Exited"]==1,"Age"], bins=25, density=True, alpha=0.6, label="Churned")
plt.xlabel("Age")
plt.ylabel("Density")
plt.title("Age Distribution by Churn")
plt.legend()
plt.grid(axis="y")
plt.show()

data0 = df.loc[df["Exited"]==0,"Balance"].values
data1 = df.loc[df["Exited"]==1,"Balance"].values
plt.figure(figsize=(7,4))
plt.boxplot([data0, data1], labels=["Stayed","Churned"], showfliers=False)
plt.ylabel("Balance")
plt.title("Balance by Churn (Boxplot)")
plt.grid(axis="y")
plt.show()

counts = df.groupby(["NumOfProducts","Exited"]).size().unstack(fill_value=0).reindex(sorted(df["NumOfProducts"].unique()))
stayed = counts.get(0, 0).values
churned = counts.get(1, 0).values
x = range(len(counts.index))
plt.figure(figsize=(7,4))
plt.bar(x, stayed, label="Stayed")
plt.bar(x, churned, bottom=stayed, label="Churned")
plt.xticks(x, counts.index)
plt.xlabel("NumOfProducts")
plt.ylabel("Count")
plt.title("NumOfProducts vs Churn (Stacked)")
plt.legend()
plt.grid(axis="y")
plt.show()

geo = ["France","Germany","Spain"]
tab = df.groupby("Geography")["Exited"].agg(["mean","count"]).reindex(geo)
rate = tab["mean"].values
stay_rate = 1 - rate
x = range(len(geo))
plt.figure(figsize=(7,4))
plt.bar(x, stay_rate, label="Stayed rate")
plt.bar(x, rate, bottom=stay_rate, label="Churn rate")
plt.xticks(x, geo)
plt.ylabel("Rate")
plt.title("Churn Rate by Geography (Stacked)")
plt.ylim(0,1)
plt.legend()
plt.grid(axis="y")
plt.show()

"""## Train–Test Split and Class Balance

To evaluate model performance reliably, the dataset is split into **training (80%)** and **testing (20%)** sets, using stratified sampling to preserve the churn distribution.

Key points:
- **Target variable**: `Exited` (0 = stayed, 1 = churned).  
- **Stratification** ensures the same churn ratio is maintained in both train and test sets.  
- Since churn cases (positive class) are fewer than non-churn, the dataset is **imbalanced**, which requires special handling in modeling (e.g., `scale_pos_weight` in XGBoost).  

"""

X_train, X_test, y_train, y_test, =train_test_split(

  X,y,
  test_size=0.20,
  stratify=y,
  random_state=42
)

print("Train class ratio:")
print((y_train.value_counts(normalize=True)*100).round(2))
print("\nTest class ratio")
print((y_test.value_counts(normalize=True)*100).round(2))

"""## Modeling with XGBoost Pipeline

For the predictive model, we use **XGBoost (Extreme Gradient Boosting)** — a powerful algorithm well-suited for imbalanced classification tasks like churn prediction.  

Key design choices:
- **ColumnTransformer**:  
  - OneHotEncoder for categorical variables (`Geography`, `Gender`)  
  - StandardScaler for numerical variables  
- **Pipeline**:  
  - Ensures preprocessing and modeling are combined into one reproducible workflow  
  - Prevents data leakage between training and testing  
- **Class imbalance handling**:  
  - The `scale_pos_weight` parameter is set based on the ratio of non-churned to churned customers  

This integrated pipeline makes the model cleaner, repeatable, and production-ready.

"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

"""## Model Evaluation

After training the XGBoost pipeline, we evaluate the model on the test set using multiple metrics:

- **Confusion Matrix**: Shows the distribution of true positives, false positives, true negatives, and false negatives.  
- **Classification Report**: Provides precision, recall, and F1-score for each class (churned vs. stayed).  
- **ROC-AUC**: Measures the ability of the model to distinguish between churned and non-churned customers.  

These metrics provide a holistic view of how well the model balances accuracy and the detection of churn cases.

"""

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

y_pred= clf.predict(X_test)
y_proba=clf.predict_proba(X_test)[:,1]


print("ROC-AUC:", roc_auc_score(y_test, y_proba))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))
print("\Confusion Matrix:\n", confusion_matrix(y_test, y_pred ))

y_probs=clf.predict_proba(X_test)[:,1]

y_pred_05=(y_probs>=0.5).astype(int)
print("ROC-AUC:", roc_auc_score(y_test, y_probs))
print("\nClassification Report (threshold=0.5):")
print(classification_report(y_test, y_pred_05))
print("\nConfusion Matrix (threshold=0.5):")
print(confusion_matrix(y_test, y_pred_05))

# Threshold = 0.4
y_pred_04 = (y_probs >= 0.4).astype(int)

print("\nClassification Report (threshold=0.4):")
print(classification_report(y_test, y_pred_04))
print("\nConfusion Matrix (threshold=0.4):")
print(confusion_matrix(y_test, y_pred_04))

# Threshold = 0.6
y_pred_06 = (y_probs >= 0.6).astype(int)

print("\nClassification Report (threshold=0.6):")
print(classification_report(y_test, y_pred_06))
print("\nConfusion Matrix (threshold=0.6):")
print(confusion_matrix(y_test, y_pred_06))

from sklearn.ensemble import RandomForestClassifier

rf=Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(
        n_estimators=200,
        max_depth= None,
        random_state=42,
        class_weight="balanced"
    ))
])

rf.fit(X_train, y_train)
y_pred=rf.predict(X_test)
y_probs=rf.predict_proba(X_test)[:,1]

print("ROC-AUC:",roc_auc_score(y_test, y_probs))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

import numpy as np
pos=(y_train==1).sum()
neg=(y_train==0).sum()
scale_pos_weight=neg/pos
scale_pos_weight

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from scipy.stats import randint, uniform

param_dist = {
    "classifier__n_estimators": randint(300, 900),
    "classifier__max_depth": randint(3, 9),
    "classifier__learning_rate": uniform(0.01, 0.15),
    "classifier__subsample": uniform(0.6, 0.4),
    "classifier__colsample_bytree": uniform(0.6, 0.4),
    "classifier__min_child_weight": randint(1, 8),
    "classifier__gamma": uniform(0.0, 0.4),
    "classifier__reg_lambda": uniform(0.5, 1.5),
    "classifier__reg_alpha": uniform(0.0, 0.5)
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rs = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_dist,
    n_iter=30,
    scoring="roc_auc",
    n_jobs=-1,
    cv=cv,
    verbose=1,
    random_state=42
)

rs.fit(X_train, y_train)

print("Best CV ROC-AUC:", rs.best_score_)
print("Best Params:")
for k, v in rs.best_params_.items():
    print(f"  {k}: {v}")

best_model = rs.best_estimator_
y_pred  = best_model.predict(X_test)
y_probs = best_model.predict_proba(X_test)[:, 1]

print("Test ROC-AUC:", roc_auc_score(y_test, y_probs))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

X = df.drop("Exited", axis=1)
y = df["Exited"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X = df.drop(columns=["RowNumber", "CustomerId", "Surname", "Exited"])
y = df["Exited"]


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier


categorical_cols = ["Geography", "Gender"]
numeric_cols = [col for col in X_train.columns if col not in categorical_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_cols),
        ("num", StandardScaler(), numeric_cols),
    ]
)

xgb_model = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("classifier", XGBClassifier(
            random_state=42,
            use_label_encoder=False,
            eval_metric="logloss",
            scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1]
        ))
    ]
)

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score


xgb_model.fit(X_train, y_train)


y_pred = xgb_model.predict(X_test)
y_probs = xgb_model.predict_proba(X_test)[:, 1]

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nROC-AUC:", roc_auc_score(y_test, y_probs))

"""## Threshold Optimization

While models output probabilities, business decisions usually need a binary label.  
Using the default threshold of **0.50** can be sub-optimal for **imbalanced** problems like churn.

In this step we:
- Plot **Precision** and **Recall** against the decision threshold to visualize the trade-off.
- Select a threshold that aligns with the business objective (e.g., prioritize recall to catch more churners, or balance with F1).
- Report performance again **at the chosen threshold** for transparency and reproducibility.

This makes the evaluation actionable: stakeholders can tune sensitivity (recall) vs. cost (precision) based on operational constraints.

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve


precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)

plt.figure(figsize=(7,5))
plt.plot(thresholds, precisions[:-1], label="Precision")
plt.plot(thresholds, recalls[:-1], label="Recall")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision-Recall vs Threshold")
plt.legend()
plt.grid()
plt.show()


target_recall = 0.75
idx = np.argmax(recalls >= target_recall)
best_threshold = thresholds[idx]

print("Selected  threshold:", best_threshold)

y_pred_thresh = (y_probs >= best_threshold).astype(int)
print("\nClassification Report (New Threshold):\n", classification_report(y_test, y_pred_thresh))

from sklearn.metrics import f1_score

f1_scores = []
for t in thresholds:
    y_pred_t = (y_probs >= t).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_t))

best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]

print("Best threshold (F1'e göre):", best_threshold)


y_pred_best = (y_probs >= best_threshold).astype(int)
print("\nClassification Report (Best threshold):\n", classification_report(y_test, y_pred_best))
print("ROC-AUC:", roc_auc_score(y_test, y_probs))

"""## Model Interpretability with SHAP

We use **SHAP (SHapley Additive exPlanations)** to explain individual predictions and the model globally:
- **What it shows**: the contribution of each feature to the churn probability for each sample.
- **Sign**: positive SHAP values push predictions toward **churn (1)**; negative values push toward **stay (0)**.
- **Global view**:
  - **Bar plot** ranks features by average absolute impact.
  - **Dot plot** shows the distribution and direction of impacts across samples.

Notes and caveats:
- SHAP attributions are **model- and data-dependent**; correlated features may split/shared importance.
- Interpret patterns, not single points: combine SHAP with domain knowledge and business context.

These insights guide actionable policies (e.g., product bundling for low-product customers, tailored offers by geography/age).

"""

import shap


explainer = shap.Explainer(xgb_model.named_steps["classifier"],
                           xgb_model.named_steps["preprocessor"].transform(X_train))

X_test_transformed = xgb_model.named_steps["preprocessor"].transform(X_test)
shap_values = explainer(X_test_transformed)


shap.summary_plot(shap_values, X_test_transformed, plot_type="bar", feature_names=xgb_model.named_steps["preprocessor"].get_feature_names_out())


shap.summary_plot(shap_values, X_test_transformed, feature_names=xgb_model.named_steps["preprocessor"].get_feature_names_out())

"""## ROC Curve

The ROC curve shows the trade-off between **True Positive Rate (TPR/Recall)** and **False Positive Rate (FPR)** across all thresholds.
- **AUC (Area Under the Curve)** summarizes overall separability: closer to **1.0** is better.
- A curve well above the diagonal baseline indicates the model is effectively distinguishing churners from non-churners.
- This plot complements Precision–Recall analysis by focusing on ranking quality rather than a single operating point.

We report the **AUC** on the test set and visualize the ROC curve to provide a standard, model-agnostic comparison metric.

"""

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC curve (AUC = {roc_auc:.3f})")
plt.plot([0,1], [0,1], color="navy", lw=2, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC Curve)")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""## Feature Importance (Table)

We report global feature importance using the **mean absolute SHAP values**.  
This table complements the SHAP plots by providing a numeric ranking that can be easily cited in reports.

"""

import pandas as pd
import numpy as np


shap_importance = np.abs(shap_values.values).mean(axis=0)


feature_names = xgb_model.named_steps["preprocessor"].get_feature_names_out()


importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Mean(|SHAP|)": shap_importance
}).sort_values(by="Mean(|SHAP|)", ascending=False)

importance_df.head(15)

plt.figure(figsize=(8,6))
plt.barh(importance_df["Feature"][:15][::-1], importance_df["Mean(|SHAP|)"][:15][::-1])
plt.xlabel("Mean(|SHAP value|)")
plt.title("Feature Importance (based on SHAP values)")
plt.grid(axis="x")
plt.show()

"""## Model Saving & Reusability

The entire preprocessing + XGBoost pipeline is saved with `joblib` for reproducible inference.  
This enables direct `predict` on new data without retraining, ensuring consistent preprocessing and model behavior.

"""

import joblib

joblib.dump(xgb_model, "churn_model.pkl")

print("Model saved as churn_model.pkl")

loaded_model = joblib.load("churn_model.pkl")


sample_pred = loaded_model.predict(X_test[:5])
print(sample_pred)